{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/alicedataset\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":2,"outputs":[{"output_type":"stream","text":"['alice.txt']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(13)\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding, Reshape, Activation, Input\nfrom keras.layers.merge import Dot\nfrom keras.utils import np_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import skipgrams\nimport numpy as np\nnp.random.seed(13)\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Reshape\nfrom IPython.display import SVG\nfrom keras.utils import np_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.preprocessing.text import Tokenizer\n# from keras.utils.visualize_util import model_to_dot, plot\n# from gensim.models.doc2vec import Word2Vec\n\nimport gensim","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/alicedataset/alice.txt'\ncorpus = open(path).readlines()\ncorpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nV = len(tokenizer.word_index) + 1\nV","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"3387"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import gensim.downloader as api","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# path=api.load(\"fake-news\")","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(path)\n# for i in path:\n#     print(i)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dim_embedddings = 128\n\n# inputs\nw_inputs = Input(shape=(1, ), dtype='int32')\nw = Embedding(V, dim_embedddings)(w_inputs)\n\n# context\nc_inputs = Input(shape=(1, ), dtype='int32')\nc  = Embedding(V, dim_embedddings)(c_inputs)\no = Dot(axes=2)([w, c])\no = Reshape((1,), input_shape=(1, 1))(o)\no = Activation('sigmoid')(o)\n\nSkipGram = Model(inputs=[w_inputs, c_inputs], outputs=o)\nSkipGram.summary()\nSkipGram.compile(loss='binary_crossentropy', optimizer='adam')\n\n","execution_count":23,"outputs":[{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            (None, 1)            0                                            \n__________________________________________________________________________________________________\ninput_6 (InputLayer)            (None, 1)            0                                            \n__________________________________________________________________________________________________\nembedding_5 (Embedding)         (None, 1, 128)       433536      input_5[0][0]                    \n__________________________________________________________________________________________________\nembedding_6 (Embedding)         (None, 1, 128)       433536      input_6[0][0]                    \n__________________________________________________________________________________________________\ndot_3 (Dot)                     (None, 1, 1)         0           embedding_5[0][0]                \n                                                                 embedding_6[0][0]                \n__________________________________________________________________________________________________\nreshape_3 (Reshape)             (None, 1)            0           dot_3[0][0]                      \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 1)            0           reshape_3[0][0]                  \n==================================================================================================\nTotal params: 867,072\nTrainable params: 867,072\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(5):\n    loss = 0.\n    for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n        data, labels = skipgrams(sequence=doc, vocabulary_size=V, window_size=5, negative_samples=5.)\n        x = [np.array(x) for x in zip(*data)]\n        y = np.array(labels, dtype=np.int32)\n        if x:\n            loss += SkipGram.train_on_batch(x, y)\n\n    print(loss)","execution_count":24,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n1081.5277689397335\n744.5550323724747\n691.9281262457371\n662.1209198981524\n634.0469603091478\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={}\n# f = open('vectors.txt' ,'w')\n# f.write('{} {}\\n'.format(V-1, dim_embedddings))\nvectors = SkipGram.get_weights()[0]\nfor word, i in tokenizer.word_index.items():\n    d[word]=list(vectors[i, :])\n# f.close()","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(d['city'])","execution_count":69,"outputs":[{"output_type":"stream","text":"[-0.038376633, 0.009719515, -0.0889027, -0.083733305, -0.0023773785, -0.033958938, -0.0529249, 0.030297158, 0.010152886, 0.061350428, 0.050730832, -0.046394154, 0.07699522, -0.055268608, -0.060266633, 0.07478753, 0.07128036, -0.0769707, 0.050335888, 0.05958583, 0.07972122, -0.018003779, -0.043756507, 0.050357487, 0.03604521, -0.046744127, 0.023621745, -0.022285206, 0.03634491, -0.031569608, -0.029099608, -0.00028811742, -0.022062441, 0.03211879, -0.028555196, 0.046905283, -0.059432924, -0.08504348, -0.009541368, 0.019290762, 0.022488536, 0.08994357, 0.053111628, -0.048452567, -0.019081004, 0.08726421, 0.091808, 0.034795526, -0.010350686, -0.066471994, -0.0062665585, 0.03392686, -0.031215921, 0.01466467, 0.016712781, -0.077040836, 0.004431684, 0.048801024, -0.056542218, -0.047240105, 0.011561346, -0.08643786, -0.0779489, -0.03215812, -0.080942474, -0.08169672, -0.072708584, -0.086208746, 0.077827804, 0.060280856, 0.0068738908, 0.048275862, -0.032527406, -0.004592836, 0.05459291, -0.011683355, -0.042511497, -0.041327957, 0.02081236, 0.06573441, 0.029924152, 0.072312206, -0.08263383, -0.013201746, -0.030988753, -0.011910785, -0.008791961, -0.007758419, -0.015172355, -0.08227096, -0.05284652, 0.050522253, 0.011188979, 0.021780917, -0.017679794, 0.0758005, 0.08203405, 0.06479257, -0.014057592, 0.010741376, -0.015380082, -0.012099393, -0.047993578, 0.08705202, 0.08238368, -0.027491229, -0.04943058, -0.052866284, 0.080675796, -0.045790195, -0.022368573, -0.01022976, 0.087674916, -0.014240847, -0.045896918, 0.031529628, -0.09094584, -0.0011080274, 0.0074571725, -0.022000799, -0.00063119136, -0.03890056, 0.038122606, 0.043611594, -0.014310633, 0.048979785, 0.015529459, -0.09264709]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_sentence_vector(words, model, num_features):\n    featureVec = np.zeros((num_features,), dtype=\"float32\")\n    nwords = 0\n    for word in words:\n        if word in model.keys():\n            featureVec = np.add(featureVec, model[word])\n\n    return featureVec","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = ['i','will','be','eating','tea']\nsentence2 = ['i','will','be','drinking','coffee']","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature1 = avg_sentence_vector(sentence1,d,128)\nfeature2 = avg_sentence_vector(sentence2,d,128)","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import cosine\nprint(cosine(feature1,feature2))","execution_count":72,"outputs":[{"output_type":"stream","text":"0.032353103160858154\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence3 = ['I','am','walking','to','America']\nsentence4 = ['I','am','going','to','Bharat']","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature3 = avg_sentence_vector(sentence3,d,128)\nfeature4 = avg_sentence_vector(sentence4,d,128)","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cosine(feature3,feature4))","execution_count":75,"outputs":[{"output_type":"stream","text":"0.05034977197647095\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import cv2","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}