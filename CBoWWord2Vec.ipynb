{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(13)\n\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Lambda\nfrom keras.utils.data_utils import get_file\nfrom keras.utils import np_utils\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\n\nimport gensim","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/alice.txt'\ncorpus = open(path).readlines()[:300]\ncorpus = [sentence for sentence in corpus if sentence.count(' ') >= 2]\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\ncorpus = tokenizer.texts_to_sequences(corpus)\nnb_samples = sum(len(s) for s in corpus)\nV = len(tokenizer.word_index) + 1\ndim = 100\nwindow_size = 2","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_data(corpus, window_size, V):\n    maxlen = window_size*2\n    for words in corpus:\n        L = len(words)\n        for index, word in enumerate(words):\n            contexts = []\n            labels   = []            \n            s = index - window_size\n            e = index + window_size + 1\n            \n            contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n            labels.append(word)\n\n            x = sequence.pad_sequences(contexts, maxlen=maxlen)\n            y = np_utils.to_categorical(labels, V)\n            yield (x, y)\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbow = Sequential()\ncbow.add(Embedding(input_dim=V, output_dim=dim, input_length=window_size*2))\ncbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\ncbow.add(Dense(V, activation='softmax'))\n","execution_count":6,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbow.compile(loss='categorical_crossentropy', optimizer='adadelta')\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ite in range(10):\n    loss = 0.\n    for x, y in generate_data(corpus, window_size, V):\n        loss += cbow.train_on_batch(x, y)\n\n    print(ite, loss)","execution_count":8,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n0 17409.690202236176\n1 16148.344410419464\n2 16017.57739496231\n3 15922.420579314232\n4 15814.984168946743\n5 15715.434266448021\n6 15628.712351620197\n7 15551.12812909484\n8 15476.529289752245\n9 15402.650278761983\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={}\n# f = open('vectors.txt' ,'w')\n# f.write('{} {}\\n'.format(V-1, dim_embedddings))\nvectors = cbow.get_weights()[0]\nfor word, i in tokenizer.word_index.items():\n    d[word]=list(vectors[i, :])\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(d.keys())","execution_count":14,"outputs":[{"output_type":"stream","text":"dict_keys(['the', 'she', 'to', 'and', 'it', 'was', 'a', 'of', '’', 'in', 'alice', 'her', 'i', 'that', 'down', 'you', 'for', 'but', 'very', 'on', 'had', 'this', 'as', 'be', 'so', 'little', 'with', 'way', 'at', 'out', 'not', 'or', 'herself', 'up', 'how', 'like', 'one', 'no', 'rabbit', 'when', 'all', 'see', 'into', 'what', 'there', 'think', 'me', 'if', 'thought', 'could', 'time', 'about', 'went', 'were', 'off', 'said', 'by', 'get', 'do', 'would', 'much', 'feet', 'through', 'door', 'alice’s', 'is', 'use', 'nothing', 'say', 'found', 'wonder', 'they', 'which', 'key', '‘and', 'shall', 'then', 'either', 'moment', 'going', 'too', '‘i', 'my', 'eat', 'ever', 'came', 'hall', 'garden', 'go', 'poor', 'once', 'well', 'suddenly', 'white', 'quite', 'looked', 'never', 'before', 'after', 'large', 'again', 'look', 'tried', 'great', 'must', 'things', 'good', 'right', 'began', 'them', 'might', 'table', 'any', 'now', 'adventures', 'wonderland', 'ebook', 'under', 'hole', 'book', 'getting', '‘oh', 'dear', 'oh', 'seemed', 'just', 'fell', 'first', 'here', 'upon', 'fall', 'got', 'people', 'soon', 'dinah', 'i’m', 'cats', 'bats', 'hand', 'round', 'golden', 'head', 'bottle', 'i’ll', 'marked', 'project', 'almost', 'away', 'gutenberg', 'having', 'pictures', 'own', 'hot', 'eyes', 'did', 'over', 'took', 'seen', 'another', 'deep', 'dark', 'from', 'put', 'such', 'even', 'i’ve', 'near', 'sort', 'nice', 'words', 'it’ll', 'rather', 'ask', 'know', 'manage', 'remember', 'felt', 'long', 'passage', 'it’s', 'low', 'other', 'trying', 'glass', 'small', 'however', 'inches', 'high', 'than', 'will', 'candle', 'more', 'tears', 'cake', 'can', 'he', 'lewis', 'carroll', 'give', 'date', 'set', 'chapter', 'tired', 'sister', 'considering', 'mind', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'trouble', 'ran', 'close', 'hear', 'late', 'ought', 'have', 'watch', 'waistcoat', 'pocket', 'hurried', 'started', 'across', 'falling', 'happen', 'make', 'coming', 'anything', 'noticed', 'cupboards', 'shelves', 'saw', 'jar', '‘well', 'they’ll', 'why', 'top', 'come', 'an', 'end', 'many', 'miles', 'somewhere', 'earth', 'let', 'four', 'several', 'though', 'still', '‘', 'that’s', 'distance', 'latitude', 'longitude', 'funny', 'seem', 'among', 'walk', 'their', 'didn’t', 'please', 'new', 'fancy', 'air', 'girl', 'perhaps', 'talking', 'should', 'wish', 'bat', 'saying', '‘do', 'sometimes', 'begun', '‘now', 'tell', 'thump', 'bit', 'sight', 'turned', 'corner', 'ears', 'behind', 'roof', 'doors', 'side', 'every', 'alas', 'rate', 'opened', 'larger', 'along', 'those', 'telescope', 'only', 'happened', 'few', 'indeed', 'waiting', 'back', 'half', 'find', 'rules', 'shutting', '‘which', '‘drink', 'me’', 'beautifully', 'hurry', 'who', 'your', 'forgotten', '‘poison', 'finding', 'fact', 'finished', 'curious', 'size', 'thing', 'reach', 'best', 'sat', 'cried', 'there’s', 'crying', 'generally', 'box', 'against', 'two', '‘to', 'eye', 'lying', 'makes', 'grow', 'happens', 'surprised', 'same', 'sure', 'pool', 'far', 'won’t', 'pair', 'kid', 'gloves', 'fan', 'duchess', 'gutenberg’s', 'anyone', 'anywhere', 'cost', 'restrictions', 'whatsoever', 'may', 'copy', 're', 'terms', 'license', 'included', 'online', 'www', 'org', 'title', 'author', 'posting', 'june', '25', '2008', '11', 'release', 'march', '1994', 'last', 'updated', 'october', '6', '2016', 'character', 'encoding', 'utf', '8', 'start', 'millennium', 'fulcrum', 'edition', '3', '0', 'beginning', 'sitting', 'bank', 'twice', 'peeped', 'reading', 'conversations', '‘without', 'day', 'pleasure', 'making', 'daisy', 'chain', 'worth', 'picking', 'daisies', 'pink', 'remarkable', 'nor', 'itself', 'afterwards', 'occurred', 'wondered', 'natural', 'actually', 'its', 'flashed', 'take', 'burning', 'curiosity', 'field', 'fortunately', 'pop', 'hedge', 'world', 'straight', 'tunnel', 'some', 'dipped', 'stopping', 'slowly', 'plenty', 'next', 'sides', 'filled', 'maps', 'hung', 'pegs', 'passed', 'labelled', '‘orange', 'marmalade’', 'disappointment', 'empty', 'drop', 'fear', 'killing', 'somebody', 'managed', 'past', '‘after', 'tumbling', 'stairs', 'brave', 'home', 'wouldn’t', 'house', 'likely', 'true', 'fallen', 'aloud', 'centre', 'thousand', 'learnt', 'lessons', 'schoolroom', 'opportunity', 'showing', 'knowledge', 'listen', 'practice', 'yes', 'idea', 'grand', 'presently', 'heads', 'downward', 'antipathies', 'glad', 'listening', 'sound', 'word', 'name', 'country', 'ma’am', 'zealand', 'australia', 'curtsey', 'spoke', 'curtseying', 'you’re', 'ignorant', 'she’ll', 'asking', 'written', 'else', '‘dinah’ll', 'miss', 'night', 'cat', 'hope', 'saucer', 'milk', 'tea', 'are', 'mice', 'afraid', 'catch', 'mouse', 'dreamy', 'couldn’t', 'answer', 'question', 'matter', 'dozing', 'dream', 'walking', 'earnestly', 'truth', 'heap', 'sticks', 'dry', 'leaves', 'hurt', 'jumped', 'overhead', 'hurrying', 'lost', 'wind', 'whiskers', 'longer', 'lit', 'row', 'lamps', 'hanging', 'locked', 'been', 'walked', 'sadly', 'middle', 'wondering', 'three', 'legged', 'solid', 'except', 'tiny', 'belong', 'locks', 'open', 'second', 'curtain', 'fifteen', 'lock', 'delight', 'fitted', 'led', 'rat', 'knelt', 'loveliest', 'longed', 'wander', 'beds', 'bright', 'flowers', 'cool', 'fountains', 'doorway', '‘it', 'without', 'shoulders', 'shut', 'knew', 'begin', 'lately', 'really', 'hoping', 'telescopes', 'certainly', 'neck', 'paper', 'label', 'printed', 'wise', '‘no', '“poison”', 'not’', 'read', 'histories', 'children', 'burnt', 'eaten', 'wild', 'beasts', 'unpleasant', 'because', 'simple', 'friends', 'taught', 'red', 'poker', 'burn', 'hold', 'cut', 'finger', 'deeply', 'knife', 'usually', 'bleeds', 'drink', 'certain', 'disagree', 'sooner', 'later', 'ventured', 'taste', 'mixed', 'flavour', 'cherry', 'tart', 'custard', 'pine', 'apple', 'roast', 'turkey', 'toffee', 'buttered', 'toast', '‘what', 'feeling', 'ten', 'face', 'brightened', 'lovely', 'waited', 'minutes', 'shrink', 'further', 'nervous', '‘for', '‘in', 'altogether', 'flame', 'blown', 'while', 'decided', 'possibly', 'plainly', 'climb', 'legs', 'slippery', '‘come', 'sharply', 'advise', 'leave', 'minute', 'gave', 'advice', 'seldom', 'followed', 'scolded', 'severely', 'bring', 'remembered', 'cheated', 'game', 'croquet', 'playing', 'child', 'fond', 'pretending', '‘but', 'pretend', 'hardly', 'enough', 'left', 'respectable', 'person', '‘eat', 'currants', 'smaller', 'creep', 'don’t', 'care', 'ate', 'anxiously', 'holding', 'growing', 'remained', 'eats', 'expecting', 'dull', 'life', 'common', 'work', 'ii', '‘curiouser', 'curiouser', 'forgot', 'speak', 'english', 'opening', 'largest', 'bye', 'shoes', 'stockings', 'dears', 'shan’t', 'able', 'deal', 'myself', 'kind', '‘or', 'want', 'boots', 'christmas', 'planning', '‘they', 'carrier', 'sending', 'presents', 'one’s', 'odd', 'directions', 'foot', 'esq', 'hearthrug', 'fender', 'love', 'nonsense', 'struck', 'nine', 'hopeless', 'cry', '‘you', 'ashamed', 'yourself', '‘a', 'stop', 'shedding', 'gallons', 'until', 'reaching', 'heard', 'pattering', 'hastily', 'dried', 'returning', 'splendidly', 'dressed', 'trotting', 'muttering', 'himself', 'savage', 'kept', 'desperate', 'ready', 'help', 'timid', 'voice', '‘if', 'sir', 'violently', 'dropped', 'skurried', 'darkness', 'hard'])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(d['voice'])","execution_count":16,"outputs":[{"output_type":"stream","text":"[0.022111446, -0.024869874, -0.05780252, -0.016599277, 0.012027467, -0.029446954, 0.027906682, 0.026086615, 0.0709466, 0.03227715, -0.07704123, -0.04758616, -0.0034261788, -0.026065022, 0.020674929, -0.014053873, -0.0070777936, -0.076414004, -0.016559005, -0.093848206, -0.0015646361, 0.09082728, -0.01663035, -0.03771302, -0.03024775, 0.010789503, -0.046133325, 0.029330084, 0.084216386, 0.025789026, -0.078797825, -0.06275805, -0.056608908, 0.039977673, 0.0178531, 0.023548504, -0.005291233, -0.05733729, 0.07807205, -0.065886825, 0.05576106, -0.04774684, 0.0030095957, 0.058414146, 0.06489876, 0.062968135, 0.07425928, -0.0098067345, 0.0001252573, -0.07118572, 0.09541443, 0.095160246, 0.027523402, -0.016431738, 0.0073803943, -0.0053849304, -0.05143182, -0.069541804, -0.011154982, 0.08308398, 0.03513116, 0.09122274, 0.020411184, -0.09466915, 0.008611854, 0.018507257, 0.030426554, 0.06386126, 0.097904675, 0.0058025965, -0.09679569, 0.06642695, -0.008525913, 0.071977615, 0.0915481, -0.053968605, 0.034779325, 0.08661178, -0.10078143, -0.048448447, -0.0041772667, -0.010721901, -0.017094793, 0.026268361, -0.097552314, 0.0839852, 0.046813272, 0.020908823, -0.050281174, 0.057744242, -0.09731312, -0.01671749, -0.0053051612, 0.024027603, 0.00711423, 0.026960876, -0.034169607, 0.06279091, -0.02110958, 0.02408993]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_sentence_vector(words, model, num_features):\n    featureVec = np.zeros((num_features,), dtype=\"float32\")\n    nwords = 0\n    for word in words:\n        if word in model.keys():\n            featureVec = np.add(featureVec, model[word])\n\n    return featureVec","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = ['i','will','be','eating','tea']\nsentence2 = ['i','will','be','drinking','coffee']","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature1 = avg_sentence_vector(sentence1,d,100)\nfeature2 = avg_sentence_vector(sentence2,d,100)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.spatial.distance import cosine\nprint(cosine(feature1,feature2))","execution_count":24,"outputs":[{"output_type":"stream","text":"0.003548562526702881\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence3 = ['I','am','walking','to','America']\nsentence4 = ['I','am','going','to','Bharat']","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature3 = avg_sentence_vector(sentence3,d,100)\nfeature4 = avg_sentence_vector(sentence4,d,100)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cosine(feature3,feature4))","execution_count":30,"outputs":[{"output_type":"stream","text":"0.02755063772201538\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}